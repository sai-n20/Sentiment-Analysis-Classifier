{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U44OYHOo83Px",
    "outputId": "d6777aac-7e50-4c14-c836-51da378156f4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Sai\n",
      "[nltk_data]     Nadkarni\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to C:\\Users\\Sai\n",
      "[nltk_data]     Nadkarni\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\Sai\n",
      "[nltk_data]     Nadkarni\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Sai Nadkarni\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package vader_lexicon to C:\\Users\\Sai\n",
      "[nltk_data]     Nadkarni\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import re\n",
    "from sklearn import *\n",
    "import time\n",
    "import re, string\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tknzr = TweetTokenizer(strip_handles=True, reduce_len=True)\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('vader_lexicon')\n",
    "stop_words = stopwords.words('english')\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk import classify\n",
    "from nltk import NaiveBayesClassifier\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import svm, tree, ensemble, linear_model\n",
    "from sklearn import metrics\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Constants\n",
    "anchorTagsRegEx = re.compile(\"<\\/*\\w+>\")\n",
    "offKeyCharRegEx = re.compile(\"[^\\sa-zA-Z]+\")\n",
    "trainingPercent = 100\n",
    "classDividerPercent = 33.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "obamaData = pd.read_excel(\"training-Obama-Romney-tweets.xlsx\", sheet_name='Obama', header=None, names=['Date', 'Time','Tweet', 'Class'], usecols=range(1,5))\n",
    "obamaData = obamaData.drop([0, 1])\n",
    "obamaData = obamaData.drop(columns=['Date', 'Time'])\n",
    "romneyData = pd.read_excel(\"training-Obama-Romney-tweets.xlsx\", sheet_name='Romney', header=None, names=['Date', 'Time','Tweet', 'Class'], usecols=range(1,5))\n",
    "romneyData = romneyData.drop([0, 1])\n",
    "romneyData = romneyData.drop(columns=['Date', 'Time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop mixed class\n",
    "obamaData[\"Class\"] = pd.to_numeric(obamaData[\"Class\"], downcast = 'integer', errors='coerce')\n",
    "romneyData[\"Class\"] = pd.to_numeric(romneyData[\"Class\"], downcast = 'integer', errors='coerce')\n",
    "obamaData = obamaData.dropna()\n",
    "romneyData = romneyData.dropna()\n",
    "obamaData = obamaData[obamaData.Class != 2]\n",
    "romneyData = romneyData[romneyData.Class != 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ototalPositiveCount = 0\n",
    "ototalNegativeCount = 0\n",
    "ototalNeutralCount = 0\n",
    "rtotalPositiveCount = 0\n",
    "rtotalNegativeCount = 0\n",
    "rtotalNeutralCount = 0\n",
    "\n",
    "for index, tweet in enumerate(obamaData['Class']):\n",
    "    if tweet == 1:\n",
    "        ototalPositiveCount += 1\n",
    "    elif tweet == -1:\n",
    "        ototalNegativeCount += 1\n",
    "    elif tweet == 0:\n",
    "        ototalNeutralCount += 1\n",
    "\n",
    "for index, tweet in enumerate(romneyData['Class']):\n",
    "    if tweet == 1:\n",
    "        rtotalPositiveCount += 1\n",
    "    elif tweet == -1:\n",
    "        rtotalNegativeCount += 1\n",
    "    elif tweet == 0:\n",
    "        rtotalNeutralCount += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(data):\n",
    "    tokenized = []\n",
    "    for i in data:\n",
    "        i = re.sub(anchorTagsRegEx, \"\", i)\n",
    "        i = re.sub(offKeyCharRegEx, \"\", i)\n",
    "        tokenized.append(tknzr.tokenize(str(i)))\n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatizeTweet(data):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatizedData = []\n",
    "    \n",
    "    for tweetTokens in data:\n",
    "        lemmatizedData.append([])\n",
    "        for word, tag in pos_tag(tweetTokens):\n",
    "            if tag.startswith('NN'):\n",
    "                pos = 'n'\n",
    "            elif tag.startswith('VB'):\n",
    "                pos = 'v'\n",
    "            else:\n",
    "                pos = 'a'\n",
    "            lemmatizedData[len(lemmatizedData) - 1].append(lemmatizer.lemmatize(word, pos))\n",
    "    return lemmatizedData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def noiseRemoval(data):\n",
    "    cleanData = []\n",
    "    \n",
    "    for tweetTokens in data:\n",
    "        cleanData.append([])\n",
    "        for token in tweetTokens:\n",
    "            token = token.lower()\n",
    "            if token.startswith(\"http\"):\n",
    "                continue\n",
    "            \n",
    "            if token not in string.punctuation and token not in stop_words and len(token) > 1:\n",
    "                cleanData[len(cleanData) - 1].append(token)\n",
    "    return cleanData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vaderfeedback(data):\n",
    "    newdata = []\n",
    "    score = 0\n",
    "    count = 0\n",
    "    for index, tweet in enumerate(data):\n",
    "        score = analyzer.polarity_scores(tweet)\n",
    "        if score['compound'] >= 0.56:\n",
    "            newdata.append(tweet + \" brilliant excellent amazing love\")\n",
    "        elif score['compound'] <= 0.03:\n",
    "            newdata.append(tweet + \" horrible poor hate absymal\")\n",
    "        else:\n",
    "            newdata.append(tweet + \" okay fine meh mediocre\")\n",
    "    return newdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preProcessTasks(data, vader):\n",
    "    if vader:\n",
    "        data = vaderfeedback(data)\n",
    "    data = tokenize(data)\n",
    "    data = lemmatizeTweet(data)\n",
    "    data = noiseRemoval(data)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "obamaCleaned = preProcessTasks(obamaData['Tweet'], True)\n",
    "romneyCleaned = preProcessTasks(romneyData['Tweet'], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pos: 19.0\n",
      "Neg: 72.0\n",
      "Neu: 23.0\n",
      "Total: 2193\n",
      "Pos: 15.0\n",
      "Neg: 70.0\n",
      "Neu: 22.0\n",
      "Total: 2636\n"
     ]
    }
   ],
   "source": [
    "positive = [\"brilliant\", \"excellent\", \"amazing\", \"love\"]\n",
    "negative = [\"horrible\", \"poor\", \"hate\", \"absymal\"]\n",
    "neutral = [\"okay\", \"fine\", \"meh\", \"mediocre\"]\n",
    "posCount = 0\n",
    "negCount = 0\n",
    "neuCount = 0\n",
    "\n",
    "for index, tweet in enumerate(obamaData['Class']):\n",
    "    if tweet == 1 and set(positive).issubset(obamaCleaned[index]):\n",
    "        posCount += 1\n",
    "    elif tweet == -1 and set(negative).issubset(obamaCleaned[index]):\n",
    "        negCount += 1\n",
    "    elif tweet == 0 and set(neutral).issubset(obamaCleaned[index]):\n",
    "        neuCount += 1\n",
    "\n",
    "print(\"Pos:\", np.round(posCount/ototalPositiveCount, 2)*100)\n",
    "print(\"Neg:\", np.round(negCount/ototalNegativeCount, 2)*100)\n",
    "print(\"Neu:\", np.round(neuCount/ototalNeutralCount, 2)*100)\n",
    "print(\"Total:\", posCount + negCount + neuCount)\n",
    "\n",
    "posCount = 0\n",
    "negCount = 0\n",
    "neuCount = 0\n",
    "for index, tweet in enumerate(romneyData['Class']):\n",
    "    if tweet == 1 and set(positive).issubset(romneyCleaned[index]):\n",
    "        posCount += 1\n",
    "    elif tweet == -1 and set(negative).issubset(romneyCleaned[index]):\n",
    "        negCount += 1\n",
    "    elif tweet == 0 and set(neutral).issubset(romneyCleaned[index]):\n",
    "        neuCount += 1\n",
    "\n",
    "print(\"Pos:\", np.round(posCount/rtotalPositiveCount, 2)*100)\n",
    "print(\"Neg:\", np.round(negCount/rtotalNegativeCount, 2)*100)\n",
    "print(\"Neu:\", np.round(neuCount/rtotalNeutralCount, 2)*100)\n",
    "print(\"Total:\", posCount + negCount + neuCount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "bothPrezCleaned = preProcessTasks(obamaData['Tweet'].append(romneyData['Tweet']), True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def overSample(data, labels):\n",
    "    data = data.toarray()\n",
    "    smote = SMOTE(random_state=871, n_jobs=-1)\n",
    "    data, labels = smote.fit_resample(data, labels)\n",
    "    return data, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize(train, test):\n",
    "    newtrain = []\n",
    "    newtest = []\n",
    "    for listitem in test:\n",
    "        newtest.append(' '.join(listitem))\n",
    "    for listitem in train:\n",
    "        newtrain.append(' '.join(listitem))\n",
    "    vec = feature_extraction.text.TfidfVectorizer(min_df = 0.00125, max_df = 0.7, sublinear_tf=True, use_idf=True, ngram_range=(1,5))\n",
    "    train_vector = vec.fit_transform(newtrain)\n",
    "    test_vector = vec.transform(newtest)\n",
    "    return train_vector, test_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "obamaX_train, obamaX_test, obamay_train, obamay_test = model_selection.train_test_split(obamaData['Tweet'], obamaData['Class'], test_size=0.4)\n",
    "obamaVector, obamaTestVector = vectorize(obamaCleaned, obamaX_test)\n",
    "obamaOverSampled, obamaOverSampledLabels = overSample(obamaVector, obamaData['Class'])\n",
    "\n",
    "romneyX_train, romneyX_test, romneyy_train, romneyy_test = model_selection.train_test_split(romneyData['Tweet'], romneyData['Class'], test_size=0.4)\n",
    "romneyVector, romneyTestVector = vectorize(romneyCleaned, romneyX_test)\n",
    "romneyOverSampled, romneyOverSampledLabels = overSample(romneyVector, romneyData['Class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "bothVector, bothTest = vectorize(bothPrezCleaned, bothPrez2Cleaned)\n",
    "bothOverSampled, bothOverSampledLabels = overSample(bothVector, obamaData['Class'].reset_index(drop=True).append(romneyData['Class'].reset_index(drop=True)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collection of classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    'Multinomial NB': MultinomialNB(),\n",
    "    'Decision Tree': tree.DecisionTreeClassifier(random_state=0),\n",
    "    'Random Forest': ensemble.RandomForestClassifier(criterion='entropy', n_jobs=-1),\n",
    "    'Logistic Regression': linear_model.LogisticRegression(max_iter=200),\n",
    "    'SVM': svm.SVC(probability=True),\n",
    "    'Ridge': linear_model.RidgeClassifier(),\n",
    "    'Stochastic Gradient Descent': linear_model.SGDClassifier(),\n",
    "    'Bagging with forest': ensemble.BaggingClassifier(base_estimator = ensemble.RandomForestClassifier(n_jobs=-1),n_jobs=-1),\n",
    "    'K-Neighbors': neighbors.KNeighborsClassifier(n_jobs=-1),\n",
    "    'AdaBoost forest': ensemble.AdaBoostClassifier(base_estimator = ensemble.RandomForestClassifier(n_jobs=-1))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifyMods(classifier, train_vectors, train_class):\n",
    "    preds = model_selection.cross_val_predict(classifier, train_vectors, train_class, cv=5, n_jobs=-1)\n",
    "    accScore = metrics.accuracy_score(train_class,preds)\n",
    "    labels = [1,-1]\n",
    "    precision = metrics.precision_score(train_class, preds, average=None,labels=labels)\n",
    "    recall = metrics.recall_score(train_class,preds,average=None,labels=labels)\n",
    "    f1score = metrics.f1_score(train_class,preds,average=None,labels=labels)\n",
    "    return accScore, precision, recall, f1score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Obama, accuracy with Multinomial NB is 0.5801719777440566\n",
      "For Romney, accuracy with Multinomial NB is 0.6201175250604909\n",
      "For both, accuracy with Multinomial NB is 0.5810189947198793\n",
      "For Obama, accuracy with Decision Tree is 0.49418310571573093\n",
      "For Romney, accuracy with Decision Tree is 0.5915428044705612\n",
      "For both, accuracy with Decision Tree is 0.5314407186449976\n",
      "For Obama, accuracy with Random Forest is 0.548811330298432\n",
      "For Romney, accuracy with Random Forest is 0.7050351422974998\n",
      "For both, accuracy with Random Forest is 0.6015223205101831\n",
      "For Obama, accuracy with Logistic Regression is 0.5835440903726185\n",
      "For Romney, accuracy with Logistic Regression is 0.6559511464454431\n",
      "For both, accuracy with Logistic Regression is 0.5932935609956799\n",
      "For Obama, accuracy with SVM is 0.6032709492497049\n",
      "For Romney, accuracy with SVM is 0.7378730268464109\n",
      "For both, accuracy with SVM is 0.6604950970307892\n",
      "For Obama, accuracy with Ridge is 0.5764626538526387\n",
      "For Romney, accuracy with Ridge is 0.6596382071667243\n",
      "For both, accuracy with Ridge is 0.5921278200644586\n",
      "For Obama, accuracy with Stochastic Gradient Descent is 0.5759568369583544\n",
      "For Romney, accuracy with Stochastic Gradient Descent is 0.6602143104044245\n",
      "For both, accuracy with Stochastic Gradient Descent is 0.5889734622505657\n",
      "For Obama, accuracy with Bagging with forest is 0.5535322879784185\n",
      "For Romney, accuracy with Bagging with forest is 0.6778430694780505\n",
      "For both, accuracy with Bagging with forest is 0.589796338202016\n",
      "For Obama, accuracy with K-Neighbors is 0.5233518799527904\n",
      "For Romney, accuracy with K-Neighbors is 0.61769789146215\n",
      "For both, accuracy with K-Neighbors is 0.5522183364191181\n",
      "For Obama, accuracy with AdaBoost forest is 0.5523520485584219\n",
      "For Romney, accuracy with AdaBoost forest is 0.7434036179283328\n",
      "For both, accuracy with AdaBoost forest is 0.6354659535075088\n"
     ]
    }
   ],
   "source": [
    "calcsO = []\n",
    "calcsR = []\n",
    "calcsBoth = []\n",
    "for index, model in enumerate(models):\n",
    "    accScore, precision, recall, f1score = classifyMods(models[model], obamaOverSampled, obamaOverSampledLabels)\n",
    "    calcsO.append({})\n",
    "    calcsO[index]['Classifier'] = model\n",
    "    calcsO[index]['Accuracy'] = accScore\n",
    "    calcsO[index]['Positive Precision'] = precision[0]\n",
    "    calcsO[index]['Negative Precision'] = precision[1]\n",
    "    calcsO[index]['Positive Recall'] = recall[0]\n",
    "    calcsO[index]['Negative Recall'] = recall[1]\n",
    "    calcsO[index]['Positive F1score'] = f1score[0]\n",
    "    calcsO[index]['Negative F1score'] = f1score[1]\n",
    "    \n",
    "    accScore, precision, recall, f1score = classifyMods(models[model], romneyOverSampled, romneyOverSampledLabels)\n",
    "    calcsR.append({})\n",
    "    calcsR[index]['Classifier'] = model\n",
    "    calcsR[index]['Accuracy'] = accScore\n",
    "    calcsR[index]['Positive Precision'] = precision[0]\n",
    "    calcsR[index]['Negative Precision'] = precision[1]\n",
    "    calcsR[index]['Positive Recall'] = recall[0]\n",
    "    calcsR[index]['Negative Recall'] = recall[1]\n",
    "    calcsR[index]['Positive F1score'] = f1score[0]\n",
    "    calcsR[index]['Negative F1score'] = f1score[1]\n",
    "    \n",
    "    accScore, precision, recall, f1score = classifyMods(models[model], bothOverSampled, bothOverSampledLabels)\n",
    "    calcsBoth.append({})\n",
    "    calcsBoth[index]['Classifier'] = model\n",
    "    calcsBoth[index]['Accuracy'] = accScore\n",
    "    calcsBoth[index]['Positive Precision'] = precision[0]\n",
    "    calcsBoth[index]['Negative Precision'] = precision[1]\n",
    "    calcsBoth[index]['Positive Recall'] = recall[0]\n",
    "    calcsBoth[index]['Negative Recall'] = recall[1]\n",
    "    calcsBoth[index]['Positive F1score'] = f1score[0]\n",
    "    calcsBoth[index]['Negative F1score'] = f1score[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "obamaCrossValResult = pd.DataFrame.from_records([s for s in calcsO])\n",
    "romneyCrossValResult = pd.DataFrame.from_records([s for s in calcsR])\n",
    "bothCrossValResult = pd.DataFrame.from_records([s for s in calcsBoth])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Classifier</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Positive Precision</th>\n",
       "      <th>Negative Precision</th>\n",
       "      <th>Positive Recall</th>\n",
       "      <th>Negative Recall</th>\n",
       "      <th>Positive F1score</th>\n",
       "      <th>Negative F1score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Multinomial NB</td>\n",
       "      <td>0.580172</td>\n",
       "      <td>0.618494</td>\n",
       "      <td>0.576795</td>\n",
       "      <td>0.673242</td>\n",
       "      <td>0.613556</td>\n",
       "      <td>0.644708</td>\n",
       "      <td>0.594608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>0.494183</td>\n",
       "      <td>0.556061</td>\n",
       "      <td>0.489775</td>\n",
       "      <td>0.556904</td>\n",
       "      <td>0.472433</td>\n",
       "      <td>0.556482</td>\n",
       "      <td>0.480947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.548811</td>\n",
       "      <td>0.623447</td>\n",
       "      <td>0.538320</td>\n",
       "      <td>0.583713</td>\n",
       "      <td>0.593323</td>\n",
       "      <td>0.602926</td>\n",
       "      <td>0.564485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.583544</td>\n",
       "      <td>0.634051</td>\n",
       "      <td>0.583088</td>\n",
       "      <td>0.655539</td>\n",
       "      <td>0.599899</td>\n",
       "      <td>0.644616</td>\n",
       "      <td>0.591374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SVM</td>\n",
       "      <td>0.603271</td>\n",
       "      <td>0.686869</td>\n",
       "      <td>0.592731</td>\n",
       "      <td>0.653515</td>\n",
       "      <td>0.643399</td>\n",
       "      <td>0.669777</td>\n",
       "      <td>0.617026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Ridge</td>\n",
       "      <td>0.576463</td>\n",
       "      <td>0.623776</td>\n",
       "      <td>0.577120</td>\n",
       "      <td>0.676783</td>\n",
       "      <td>0.571573</td>\n",
       "      <td>0.649199</td>\n",
       "      <td>0.574333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Stochastic Gradient Descent</td>\n",
       "      <td>0.575957</td>\n",
       "      <td>0.626999</td>\n",
       "      <td>0.585950</td>\n",
       "      <td>0.674254</td>\n",
       "      <td>0.556904</td>\n",
       "      <td>0.649768</td>\n",
       "      <td>0.571058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Bagging with forest</td>\n",
       "      <td>0.553532</td>\n",
       "      <td>0.619273</td>\n",
       "      <td>0.549230</td>\n",
       "      <td>0.594841</td>\n",
       "      <td>0.595346</td>\n",
       "      <td>0.606811</td>\n",
       "      <td>0.571359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>K-Neighbors</td>\n",
       "      <td>0.523352</td>\n",
       "      <td>0.527506</td>\n",
       "      <td>0.549907</td>\n",
       "      <td>0.708144</td>\n",
       "      <td>0.448660</td>\n",
       "      <td>0.604621</td>\n",
       "      <td>0.494150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>AdaBoost forest</td>\n",
       "      <td>0.552352</td>\n",
       "      <td>0.606061</td>\n",
       "      <td>0.549368</td>\n",
       "      <td>0.627213</td>\n",
       "      <td>0.593829</td>\n",
       "      <td>0.616455</td>\n",
       "      <td>0.570734</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Classifier  Accuracy  Positive Precision  \\\n",
       "0               Multinomial NB  0.580172            0.618494   \n",
       "1                Decision Tree  0.494183            0.556061   \n",
       "2                Random Forest  0.548811            0.623447   \n",
       "3          Logistic Regression  0.583544            0.634051   \n",
       "4                          SVM  0.603271            0.686869   \n",
       "5                        Ridge  0.576463            0.623776   \n",
       "6  Stochastic Gradient Descent  0.575957            0.626999   \n",
       "7          Bagging with forest  0.553532            0.619273   \n",
       "8                  K-Neighbors  0.523352            0.527506   \n",
       "9              AdaBoost forest  0.552352            0.606061   \n",
       "\n",
       "   Negative Precision  Positive Recall  Negative Recall  Positive F1score  \\\n",
       "0            0.576795         0.673242         0.613556          0.644708   \n",
       "1            0.489775         0.556904         0.472433          0.556482   \n",
       "2            0.538320         0.583713         0.593323          0.602926   \n",
       "3            0.583088         0.655539         0.599899          0.644616   \n",
       "4            0.592731         0.653515         0.643399          0.669777   \n",
       "5            0.577120         0.676783         0.571573          0.649199   \n",
       "6            0.585950         0.674254         0.556904          0.649768   \n",
       "7            0.549230         0.594841         0.595346          0.606811   \n",
       "8            0.549907         0.708144         0.448660          0.604621   \n",
       "9            0.549368         0.627213         0.593829          0.616455   \n",
       "\n",
       "   Negative F1score  \n",
       "0          0.594608  \n",
       "1          0.480947  \n",
       "2          0.564485  \n",
       "3          0.591374  \n",
       "4          0.617026  \n",
       "5          0.574333  \n",
       "6          0.571058  \n",
       "7          0.571359  \n",
       "8          0.494150  \n",
       "9          0.570734  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obamaCrossValResult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Classifier</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Positive Precision</th>\n",
       "      <th>Negative Precision</th>\n",
       "      <th>Positive Recall</th>\n",
       "      <th>Negative Recall</th>\n",
       "      <th>Positive F1score</th>\n",
       "      <th>Negative F1score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Multinomial NB</td>\n",
       "      <td>0.620118</td>\n",
       "      <td>0.664790</td>\n",
       "      <td>0.612636</td>\n",
       "      <td>0.775320</td>\n",
       "      <td>0.563083</td>\n",
       "      <td>0.715813</td>\n",
       "      <td>0.586816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>0.591543</td>\n",
       "      <td>0.692602</td>\n",
       "      <td>0.540889</td>\n",
       "      <td>0.689250</td>\n",
       "      <td>0.580712</td>\n",
       "      <td>0.690922</td>\n",
       "      <td>0.560093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.705035</td>\n",
       "      <td>0.797672</td>\n",
       "      <td>0.626961</td>\n",
       "      <td>0.805392</td>\n",
       "      <td>0.745938</td>\n",
       "      <td>0.801514</td>\n",
       "      <td>0.681294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.655951</td>\n",
       "      <td>0.716441</td>\n",
       "      <td>0.638522</td>\n",
       "      <td>0.787764</td>\n",
       "      <td>0.585551</td>\n",
       "      <td>0.750412</td>\n",
       "      <td>0.610891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SVM</td>\n",
       "      <td>0.737873</td>\n",
       "      <td>0.859358</td>\n",
       "      <td>0.628611</td>\n",
       "      <td>0.823712</td>\n",
       "      <td>0.782233</td>\n",
       "      <td>0.841158</td>\n",
       "      <td>0.697058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Ridge</td>\n",
       "      <td>0.659638</td>\n",
       "      <td>0.709967</td>\n",
       "      <td>0.642143</td>\n",
       "      <td>0.819910</td>\n",
       "      <td>0.555133</td>\n",
       "      <td>0.760988</td>\n",
       "      <td>0.595476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Stochastic Gradient Descent</td>\n",
       "      <td>0.660214</td>\n",
       "      <td>0.718494</td>\n",
       "      <td>0.643055</td>\n",
       "      <td>0.817836</td>\n",
       "      <td>0.561701</td>\n",
       "      <td>0.764953</td>\n",
       "      <td>0.599631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Bagging with forest</td>\n",
       "      <td>0.677843</td>\n",
       "      <td>0.756393</td>\n",
       "      <td>0.608326</td>\n",
       "      <td>0.797442</td>\n",
       "      <td>0.717249</td>\n",
       "      <td>0.776376</td>\n",
       "      <td>0.658312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>K-Neighbors</td>\n",
       "      <td>0.617698</td>\n",
       "      <td>0.641434</td>\n",
       "      <td>0.694698</td>\n",
       "      <td>0.890425</td>\n",
       "      <td>0.262703</td>\n",
       "      <td>0.745694</td>\n",
       "      <td>0.381239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>AdaBoost forest</td>\n",
       "      <td>0.743404</td>\n",
       "      <td>0.838854</td>\n",
       "      <td>0.670581</td>\n",
       "      <td>0.840304</td>\n",
       "      <td>0.730384</td>\n",
       "      <td>0.839579</td>\n",
       "      <td>0.699206</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Classifier  Accuracy  Positive Precision  \\\n",
       "0               Multinomial NB  0.620118            0.664790   \n",
       "1                Decision Tree  0.591543            0.692602   \n",
       "2                Random Forest  0.705035            0.797672   \n",
       "3          Logistic Regression  0.655951            0.716441   \n",
       "4                          SVM  0.737873            0.859358   \n",
       "5                        Ridge  0.659638            0.709967   \n",
       "6  Stochastic Gradient Descent  0.660214            0.718494   \n",
       "7          Bagging with forest  0.677843            0.756393   \n",
       "8                  K-Neighbors  0.617698            0.641434   \n",
       "9              AdaBoost forest  0.743404            0.838854   \n",
       "\n",
       "   Negative Precision  Positive Recall  Negative Recall  Positive F1score  \\\n",
       "0            0.612636         0.775320         0.563083          0.715813   \n",
       "1            0.540889         0.689250         0.580712          0.690922   \n",
       "2            0.626961         0.805392         0.745938          0.801514   \n",
       "3            0.638522         0.787764         0.585551          0.750412   \n",
       "4            0.628611         0.823712         0.782233          0.841158   \n",
       "5            0.642143         0.819910         0.555133          0.760988   \n",
       "6            0.643055         0.817836         0.561701          0.764953   \n",
       "7            0.608326         0.797442         0.717249          0.776376   \n",
       "8            0.694698         0.890425         0.262703          0.745694   \n",
       "9            0.670581         0.840304         0.730384          0.839579   \n",
       "\n",
       "   Negative F1score  \n",
       "0          0.586816  \n",
       "1          0.560093  \n",
       "2          0.681294  \n",
       "3          0.610891  \n",
       "4          0.697058  \n",
       "5          0.595476  \n",
       "6          0.599631  \n",
       "7          0.658312  \n",
       "8          0.381239  \n",
       "9          0.699206  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "romneyCrossValResult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Classifier</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Positive Precision</th>\n",
       "      <th>Negative Precision</th>\n",
       "      <th>Positive Recall</th>\n",
       "      <th>Negative Recall</th>\n",
       "      <th>Positive F1score</th>\n",
       "      <th>Negative F1score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Multinomial NB</td>\n",
       "      <td>0.581019</td>\n",
       "      <td>0.623872</td>\n",
       "      <td>0.560640</td>\n",
       "      <td>0.682781</td>\n",
       "      <td>0.569636</td>\n",
       "      <td>0.651999</td>\n",
       "      <td>0.565102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>0.531441</td>\n",
       "      <td>0.584858</td>\n",
       "      <td>0.521281</td>\n",
       "      <td>0.618186</td>\n",
       "      <td>0.519029</td>\n",
       "      <td>0.601060</td>\n",
       "      <td>0.520153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.601522</td>\n",
       "      <td>0.679651</td>\n",
       "      <td>0.561120</td>\n",
       "      <td>0.689159</td>\n",
       "      <td>0.622300</td>\n",
       "      <td>0.684372</td>\n",
       "      <td>0.590129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.593294</td>\n",
       "      <td>0.633535</td>\n",
       "      <td>0.575124</td>\n",
       "      <td>0.691010</td>\n",
       "      <td>0.548858</td>\n",
       "      <td>0.661025</td>\n",
       "      <td>0.561684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SVM</td>\n",
       "      <td>0.660495</td>\n",
       "      <td>0.752623</td>\n",
       "      <td>0.584962</td>\n",
       "      <td>0.737914</td>\n",
       "      <td>0.683398</td>\n",
       "      <td>0.745196</td>\n",
       "      <td>0.630361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Ridge</td>\n",
       "      <td>0.592128</td>\n",
       "      <td>0.630606</td>\n",
       "      <td>0.570995</td>\n",
       "      <td>0.708702</td>\n",
       "      <td>0.541864</td>\n",
       "      <td>0.667377</td>\n",
       "      <td>0.556048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Stochastic Gradient Descent</td>\n",
       "      <td>0.588973</td>\n",
       "      <td>0.608808</td>\n",
       "      <td>0.575960</td>\n",
       "      <td>0.725159</td>\n",
       "      <td>0.549064</td>\n",
       "      <td>0.661910</td>\n",
       "      <td>0.562191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Bagging with forest</td>\n",
       "      <td>0.589796</td>\n",
       "      <td>0.661344</td>\n",
       "      <td>0.554616</td>\n",
       "      <td>0.690187</td>\n",
       "      <td>0.606871</td>\n",
       "      <td>0.675458</td>\n",
       "      <td>0.579568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>K-Neighbors</td>\n",
       "      <td>0.552218</td>\n",
       "      <td>0.559902</td>\n",
       "      <td>0.611642</td>\n",
       "      <td>0.800864</td>\n",
       "      <td>0.276692</td>\n",
       "      <td>0.659049</td>\n",
       "      <td>0.381020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>AdaBoost forest</td>\n",
       "      <td>0.635466</td>\n",
       "      <td>0.714572</td>\n",
       "      <td>0.581504</td>\n",
       "      <td>0.732360</td>\n",
       "      <td>0.642872</td>\n",
       "      <td>0.723357</td>\n",
       "      <td>0.610650</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Classifier  Accuracy  Positive Precision  \\\n",
       "0               Multinomial NB  0.581019            0.623872   \n",
       "1                Decision Tree  0.531441            0.584858   \n",
       "2                Random Forest  0.601522            0.679651   \n",
       "3          Logistic Regression  0.593294            0.633535   \n",
       "4                          SVM  0.660495            0.752623   \n",
       "5                        Ridge  0.592128            0.630606   \n",
       "6  Stochastic Gradient Descent  0.588973            0.608808   \n",
       "7          Bagging with forest  0.589796            0.661344   \n",
       "8                  K-Neighbors  0.552218            0.559902   \n",
       "9              AdaBoost forest  0.635466            0.714572   \n",
       "\n",
       "   Negative Precision  Positive Recall  Negative Recall  Positive F1score  \\\n",
       "0            0.560640         0.682781         0.569636          0.651999   \n",
       "1            0.521281         0.618186         0.519029          0.601060   \n",
       "2            0.561120         0.689159         0.622300          0.684372   \n",
       "3            0.575124         0.691010         0.548858          0.661025   \n",
       "4            0.584962         0.737914         0.683398          0.745196   \n",
       "5            0.570995         0.708702         0.541864          0.667377   \n",
       "6            0.575960         0.725159         0.549064          0.661910   \n",
       "7            0.554616         0.690187         0.606871          0.675458   \n",
       "8            0.611642         0.800864         0.276692          0.659049   \n",
       "9            0.581504         0.732360         0.642872          0.723357   \n",
       "\n",
       "   Negative F1score  \n",
       "0          0.565102  \n",
       "1          0.520153  \n",
       "2          0.590129  \n",
       "3          0.561684  \n",
       "4          0.630361  \n",
       "5          0.556048  \n",
       "6          0.562191  \n",
       "7          0.579568  \n",
       "8          0.381020  \n",
       "9          0.610650  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bothCrossValResult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotter():\n",
    "    N = 9\n",
    "    ind = np.arange(N) \n",
    "    width = 0.3\n",
    "    plt.figure(figsize=(12,8))\n",
    "    plt.grid(zorder=0)\n",
    "    xvals = obamaCrossValResult['Accuracy'].tolist()\n",
    "    bar1 = plt.bar(ind, xvals, width, color = '#4c72b0',zorder=3)\n",
    "  \n",
    "    yvals = romneyCrossValResult['Accuracy'].tolist()\n",
    "    bar2 = plt.bar(ind+width, yvals, width, color='#c44e52',zorder=3)\n",
    "  \n",
    "    zvals = bothCrossValResult['Accuracy'].tolist()\n",
    "    bar3 = plt.bar(ind+width*2, zvals, width, color = '#55a868',zorder=3)\n",
    "\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title(\"Model accuracies\")\n",
    "    plt.ylim((0.0, 0.8))\n",
    "    plt.yticks(np.arange(0.0,0.8,0.05))\n",
    "    plt.xticks(ind+width, ['MNB', 'Tree', 'Forest', 'LR', 'SVM', 'Ridge', 'SGD', 'Bagging', 'AdaBoost'])\n",
    "    plt.legend((bar1, bar2, bar3), ('Obama', 'Romney', 'Both'), loc='upper left', bbox_to_anchor=(1.01, 1))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'obamaCrossValResult' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-33-1e02bbab67d2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mplotter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-32-fcdf32babe65>\u001b[0m in \u001b[0;36mplotter\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mplotter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobamaCrossValResult\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Classifier'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# the label locations\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0mwidth\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.35\u001b[0m  \u001b[1;31m# the width of the bars\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mfig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0max\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m8\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdpi\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m80\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'obamaCrossValResult' is not defined"
     ]
    }
   ],
   "source": [
    "plotter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done first\n",
      "Wall time: 32min 19s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "VotingClassifier(estimators=[('nbm', MultinomialNB()),\n",
       "                             ('rf',\n",
       "                              RandomForestClassifier(criterion='entropy',\n",
       "                                                     n_jobs=-1)),\n",
       "                             ('lr', LogisticRegression(max_iter=200)),\n",
       "                             ('svc', SVC(probability=True)),\n",
       "                             ('rc', RidgeClassifier()),\n",
       "                             ('sgd', SGDClassifier()),\n",
       "                             ('bgf',\n",
       "                              BaggingClassifier(base_estimator=RandomForestClassifier(n_jobs=-1),\n",
       "                                                n_jobs=-1)),\n",
       "                             ('adf',\n",
       "                              AdaBoostClassifier(base_estimator=RandomForestClassifier(n_jobs=-1)))],\n",
       "                 n_jobs=-1)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "eclf = ensemble.VotingClassifier(estimators=[\n",
    "            ('nbm', models['Multinomial NB']),\n",
    "            ('rf', models['Random Forest']),\n",
    "            ('lr', models['Logistic Regression']),\n",
    "            ('svc', models['SVM']),\n",
    "            ('rc', models['Ridge']),\n",
    "            ('sgd', models['Stochastic Gradient Descent']),\n",
    "            ('bgf', models['Bagging with forest']),\n",
    "            ('adf', models['AdaBoost forest'])\n",
    "        ], voting='hard')\n",
    "\n",
    "eclfR = ensemble.VotingClassifier(estimators=[\n",
    "            ('nbm', models['Multinomial NB']),\n",
    "            ('rf', models['Random Forest']),\n",
    "            ('lr', models['Logistic Regression']),\n",
    "            ('svc', models['SVM']),\n",
    "            ('rc', models['Ridge']),\n",
    "            ('sgd', models['Stochastic Gradient Descent']),\n",
    "            ('bgf', models['Bagging with forest']),\n",
    "            ('adf', models['AdaBoost forest'])\n",
    "        ], voting='hard', n_jobs = -1)\n",
    "\n",
    "eclf.fit(bothOverSampled, bothOverSampledLabels)\n",
    "eclfR.fit(romneyOverSampled, romneyOverSampledLabels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input length Obama: 1951\n",
      "Input length Romney: 1900\n",
      "Prediction length Obama: 1951\n",
      "Prediction length Romney: 1900\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "only integer scalar arrays can be converted to a scalar index",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-32-dee52d0b6e2f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"obama.txt\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"w+\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mindex\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtestPredsO\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m         \u001b[0mout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\";;\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtestPredsO\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: only integer scalar arrays can be converted to a scalar index"
     ]
    }
   ],
   "source": [
    "#Read data\n",
    "testFileObama = pd.read_excel(\"final-testData-no-label-Obama-tweets.xlsx\", sheet_name='Obama', header=None, names=['Tweet'])\n",
    "testFileRomney = pd.read_excel(\"final-testData-no-label-Romney-tweets.xlsx\", sheet_name='Romney', header=None, names=['Tweet'])\n",
    "\n",
    "#Clean data\n",
    "obamaTestCleaned = preProcessTasks(testFileObama['Tweet'], True)\n",
    "romneyTestCleaned = preProcessTasks(testFileRomney['Tweet'], True)\n",
    "\n",
    "#Vectorize data\n",
    "obamaVector, obamaTestVector = vectorize(trainBothCleaned, obamaTestCleaned)\n",
    "romneyVector, romneyTestVector = vectorize(trainRomneyCleaned, romneyTestCleaned)\n",
    "\n",
    "#Predict class\n",
    "testPredsO = eclf.predict(obamaTestVector.toarray())\n",
    "testPredsR = eclfR.predict(romneyTestVector.toarray())\n",
    "\n",
    "print(\"Input length Obama:\", len(testFileObama))\n",
    "print(\"Input length Romney:\", len(testFileRomney))\n",
    "print(\"Prediction length Obama:\", len(testPredsO))\n",
    "print(\"Prediction length Romney:\", len(testPredsR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predict class\n",
    "testO = eclf.predict(obamaTestVector.toarray())\n",
    "testR = eclfR.predict(romneyTestVector.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function TextIOWrapper.close()>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"obama.txt\", \"w+\") as out:\n",
    "    for index in range(len(testPredsO)):\n",
    "        out.write(\"{};;{}\\n\".format(index+1, int(testPredsO[index])))\n",
    "out.close\n",
    "\n",
    "with open(\"romney.txt\", \"w+\") as out:\n",
    "    for index in range(len(testPredsR)):\n",
    "        out.write(\"{};;{}\\n\".format(index+1, int(testPredsR[index])))\n",
    "out.close"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "CS583 Final Project.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
